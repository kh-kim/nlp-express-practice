{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization using Huggingface Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khkim/miniconda3/envs/nlp_exp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f2 ./data/ratings_train.tsv > ./data/ratings_train.content.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    \"./data/ratings_train.content.txt\",\n",
    "]\n",
    "vocab_size = 48000\n",
    "min_frequency = 2\n",
    "\n",
    "output_name = f\"nsmc_bbpe_{vocab_size}\"\n",
    "output_dir = \"./tokenizers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_tokens = [f\"<unused_{i}>\" for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=train_files,\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",  # padding\n",
    "        \"<s>\",    # start of sentence\n",
    "        \"</s>\",   # end of sentence\n",
    "        \"<unk>\",  # unknown\n",
    "    ] + unused_tokens,\n",
    ")\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, output_name), exist_ok=True)\n",
    "tokenizer.save(os.path.join(output_dir, output_name, \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer(tokenizer, skip_special_tokens=True):\n",
    "    ko_sentence = \"<s>이것은 테스트 문장입니다. <unused_0>어떻게 보이나요?<unused_1> 고유명사 \\\"파이썬 파이토치 허깅페이스\\\"는 어떻게 되나요?</s>\"\n",
    "    en_sentence = \"<s>This is a test sentence. <unused_0>How does it look?<unused_1> Proper nouns \\\"Python PyTorch HuggingFace\\\" how does it go?</s>\"\n",
    "\n",
    "    print(ko_sentence)\n",
    "    if isinstance(tokenizer, ByteLevelBPETokenizer):\n",
    "        print(\">>>\", tokenizer.encode(ko_sentence).ids)\n",
    "        print(\">>>\", tokenizer.encode(ko_sentence).tokens)\n",
    "        print(\">>>\", tokenizer.decode(tokenizer.encode(ko_sentence).ids, skip_special_tokens=skip_special_tokens))\n",
    "    else:\n",
    "        print(\">>>\", tokenizer.encode(ko_sentence))\n",
    "        print(\">>>\", tokenizer.tokenize(ko_sentence))\n",
    "        print(\">>>\", tokenizer.decode(tokenizer.encode(ko_sentence), skip_special_tokens=skip_special_tokens))\n",
    "    print(en_sentence)\n",
    "    if isinstance(tokenizer, ByteLevelBPETokenizer):\n",
    "        print(\">>>\", tokenizer.encode(en_sentence).ids)\n",
    "        print(\">>>\", tokenizer.encode(en_sentence).tokens)\n",
    "        print(\">>>\", tokenizer.decode(tokenizer.encode(en_sentence).ids, skip_special_tokens=skip_special_tokens))\n",
    "    else:\n",
    "        print(\">>>\", tokenizer.encode(en_sentence))\n",
    "        print(\">>>\", tokenizer.tokenize(en_sentence))\n",
    "        print(\">>>\", tokenizer.decode(tokenizer.encode(en_sentence), skip_special_tokens=skip_special_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize with Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>이것은 테스트 문장입니다. <unused_0>어떻게 보이나요?<unused_1> 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?</s>\n",
      ">>> [1, 13158, 17621, 2372, 519, 932, 117, 324, 4, 3392, 20154, 423, 134, 5, 900, 14334, 635, 324, 105, 2845, 13796, 6164, 618, 604, 1255, 462, 331, 17556, 105, 373, 1704, 41246, 134, 2]\n",
      ">>> ['<s>', 'ìĿ´ê²ĥìĿĢ', 'ĠíħĮìĬ¤íĬ¸', 'Ġë¬¸', 'ìŀ¥', 'ìŀħëĭĪëĭ¤', '.', 'Ġ', '<unused_0>', 'ìĸ´ëĸ»ê²Į', 'Ġë³´ìĿ´ëĤĺ', 'ìļĶ', '?', '<unused_1>', 'Ġê³ł', 'ìľłëªħ', 'ìĤ¬', 'Ġ', '\"', 'íĮĮìĿ´', 'ìį¬', 'ĠíĮĮìĿ´', 'íĨł', 'ì¹ĺ', 'ĠíĹĪ', 'ê¹', 'ħ', 'íİĺìĿ´ìĬ¤', '\"', 'ëĬĶ', 'Ġìĸ´ëĸ»ê²Į', 'ĠëĲĺëĤĺìļĶ', '?', '</s>']\n",
      ">>> 이것은 테스트 문장입니다. 어떻게 보이나요? 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?\n",
      "<s>This is a test sentence. <unused_0>How does it look?<unused_1> Proper nouns \"Python PyTorch HuggingFace\" how does it go?</s>\n",
      ">>> [1, 30227, 8451, 3454, 2320, 6679, 2771, 13113, 5650, 10287, 117, 324, 4, 143, 7308, 21844, 7940, 12827, 6683, 42645, 134, 5, 6682, 185, 8336, 2526, 6917, 3977, 181, 186, 324, 105, 151, 192, 14234, 4440, 6682, 192, 155, 3676, 9945, 10292, 188, 15065, 4812, 141, 24649, 105, 8923, 7308, 21844, 7940, 12827, 20886, 134, 2]\n",
      ">>> ['<s>', 'This', 'Ġis', 'Ġa', 'Ġt', 'est', 'Ġs', 'ent', 'en', 'ce', '.', 'Ġ', '<unused_0>', 'H', 'ow', 'Ġdo', 'es', 'Ġit', 'Ġl', 'ook', '?', '<unused_1>', 'ĠP', 'r', 'op', 'er', 'Ġn', 'ou', 'n', 's', 'Ġ', '\"', 'P', 'y', 'th', 'on', 'ĠP', 'y', 'T', 'or', 'ch', 'ĠH', 'u', 'gg', 'ing', 'F', 'ace', '\"', 'Ġh', 'ow', 'Ġdo', 'es', 'Ġit', 'Ġgo', '?', '</s>']\n",
      ">>> This is a test sentence. How does it look? Proper nouns \"Python PyTorch HuggingFace\" how does it go?\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer(tokenizer, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize without Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>이것은 테스트 문장입니다. <unused_0>어떻게 보이나요?<unused_1> 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?</s>\n",
      ">>> [1, 13158, 17621, 2372, 519, 932, 117, 324, 4, 3392, 20154, 423, 134, 5, 900, 14334, 635, 324, 105, 2845, 13796, 6164, 618, 604, 1255, 462, 331, 17556, 105, 373, 1704, 41246, 134, 2]\n",
      ">>> ['<s>', 'ìĿ´ê²ĥìĿĢ', 'ĠíħĮìĬ¤íĬ¸', 'Ġë¬¸', 'ìŀ¥', 'ìŀħëĭĪëĭ¤', '.', 'Ġ', '<unused_0>', 'ìĸ´ëĸ»ê²Į', 'Ġë³´ìĿ´ëĤĺ', 'ìļĶ', '?', '<unused_1>', 'Ġê³ł', 'ìľłëªħ', 'ìĤ¬', 'Ġ', '\"', 'íĮĮìĿ´', 'ìį¬', 'ĠíĮĮìĿ´', 'íĨł', 'ì¹ĺ', 'ĠíĹĪ', 'ê¹', 'ħ', 'íİĺìĿ´ìĬ¤', '\"', 'ëĬĶ', 'Ġìĸ´ëĸ»ê²Į', 'ĠëĲĺëĤĺìļĶ', '?', '</s>']\n",
      ">>> <s>이것은 테스트 문장입니다. <unused_0>어떻게 보이나요?<unused_1> 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?</s>\n",
      "<s>This is a test sentence. <unused_0>How does it look?<unused_1> Proper nouns \"Python PyTorch HuggingFace\" how does it go?</s>\n",
      ">>> [1, 30227, 8451, 3454, 2320, 6679, 2771, 13113, 5650, 10287, 117, 324, 4, 143, 7308, 21844, 7940, 12827, 6683, 42645, 134, 5, 6682, 185, 8336, 2526, 6917, 3977, 181, 186, 324, 105, 151, 192, 14234, 4440, 6682, 192, 155, 3676, 9945, 10292, 188, 15065, 4812, 141, 24649, 105, 8923, 7308, 21844, 7940, 12827, 20886, 134, 2]\n",
      ">>> ['<s>', 'This', 'Ġis', 'Ġa', 'Ġt', 'est', 'Ġs', 'ent', 'en', 'ce', '.', 'Ġ', '<unused_0>', 'H', 'ow', 'Ġdo', 'es', 'Ġit', 'Ġl', 'ook', '?', '<unused_1>', 'ĠP', 'r', 'op', 'er', 'Ġn', 'ou', 'n', 's', 'Ġ', '\"', 'P', 'y', 'th', 'on', 'ĠP', 'y', 'T', 'or', 'ch', 'ĠH', 'u', 'gg', 'ing', 'F', 'ace', '\"', 'Ġh', 'ow', 'Ġdo', 'es', 'Ġit', 'Ġgo', '?', '</s>']\n",
      ">>> <s>This is a test sentence. <unused_0>How does it look?<unused_1> Proper nouns \"Python PyTorch HuggingFace\" how does it go?</s>\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer(tokenizer, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize with Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>이것은 테스트 문장입니다. <unused_0>어떻게 보이나요?<unused_1> 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?</s>\n",
      ">>> [1, 13158, 17621, 2372, 519, 932, 117, 324, 4, 3392, 20154, 423, 134, 5, 900, 14334, 635, 324, 105, 2845, 13796, 6164, 618, 604, 1255, 462, 331, 17556, 105, 373, 1704, 41246, 134, 2]\n",
      ">>> ['<s>', 'ìĿ´ê²ĥìĿĢ', 'ĠíħĮìĬ¤íĬ¸', 'Ġë¬¸', 'ìŀ¥', 'ìŀħëĭĪëĭ¤', '.', 'Ġ', '<unused_0>', 'ìĸ´ëĸ»ê²Į', 'Ġë³´ìĿ´ëĤĺ', 'ìļĶ', '?', '<unused_1>', 'Ġê³ł', 'ìľłëªħ', 'ìĤ¬', 'Ġ', '\"', 'íĮĮìĿ´', 'ìį¬', 'ĠíĮĮìĿ´', 'íĨł', 'ì¹ĺ', 'ĠíĹĪ', 'ê¹', 'ħ', 'íİĺìĿ´ìĬ¤', '\"', 'ëĬĶ', 'Ġìĸ´ëĸ»ê²Į', 'ĠëĲĺëĤĺìļĶ', '?', '</s>']\n",
      ">>> 이것은 테스트 문장입니다. 어떻게 보이나요? 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?\n",
      "<s>This is a test sentence. <unused_0>How does it look?<unused_1> Proper nouns \"Python PyTorch HuggingFace\" how does it go?</s>\n",
      ">>> [1, 30227, 8451, 3454, 2320, 6679, 2771, 13113, 5650, 10287, 117, 324, 4, 143, 7308, 21844, 7940, 12827, 6683, 42645, 134, 5, 6682, 185, 8336, 2526, 6917, 3977, 181, 186, 324, 105, 151, 192, 14234, 4440, 6682, 192, 155, 3676, 9945, 10292, 188, 15065, 4812, 141, 24649, 105, 8923, 7308, 21844, 7940, 12827, 20886, 134, 2]\n",
      ">>> ['<s>', 'This', 'Ġis', 'Ġa', 'Ġt', 'est', 'Ġs', 'ent', 'en', 'ce', '.', 'Ġ', '<unused_0>', 'H', 'ow', 'Ġdo', 'es', 'Ġit', 'Ġl', 'ook', '?', '<unused_1>', 'ĠP', 'r', 'op', 'er', 'Ġn', 'ou', 'n', 's', 'Ġ', '\"', 'P', 'y', 'th', 'on', 'ĠP', 'y', 'T', 'or', 'ch', 'ĠH', 'u', 'gg', 'ing', 'F', 'ace', '\"', 'Ġh', 'ow', 'Ġdo', 'es', 'Ġit', 'Ġgo', '?', '</s>']\n",
      ">>> This is a test sentence. How does it look? Proper nouns \"Python PyTorch HuggingFace\" how does it go?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(os.path.join(output_dir, output_name))\n",
    "\n",
    "test_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": \"1.0\",\n",
      "  \"truncation\": null,\n",
      "  \"padding\": null,\n",
      "  \"added_tokens\": [\n",
      "    {\n",
      "      \"id\": 0,\n",
      "      \"content\": \"<pad>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"content\": \"<s>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"content\": \"</s>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"content\": \"<unk>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"content\": \"<unused_0>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"content\": \"<unused_1>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"content\": \"<unused_2>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"content\": \"<unused_3>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"content\": \"<unused_4>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"content\": \"<unused_5>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"content\": \"<unused_6>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 11,\n",
      "      \"content\": \"<unused_7>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 12,\n",
      "      \"content\": \"<unused_8>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 13,\n",
      "      \"content\": \"<unused_9>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 14,\n",
      "      \"content\": \"<unused_10>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 15,\n",
      "      \"content\": \"<unused_11>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 16,\n",
      "      \"content\": \"<unused_12>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 17,\n",
      "      \"content\": \"<unused_13>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 18,\n",
      "      \"content\": \"<unused_14>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 19,\n",
      "      \"content\": \"<unused_15>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 20,\n",
      "      \"content\": \"<unused_16>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 21,\n",
      "      \"content\": \"<unused_17>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 22,\n",
      "      \"content\": \"<unused_18>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 23,\n",
      "      \"content\": \"<unused_19>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 24,\n",
      "      \"content\": \"<unused_20>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 25,\n",
      "      \"content\": \"<unused_21>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 26,\n",
      "      \"content\": \"<unused_22>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 27,\n",
      "      \"content\": \"<unused_23>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 28,\n",
      "      \"content\": \"<unused_24>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 29,\n",
      "      \"content\": \"<unused_25>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 30,\n",
      "      \"content\": \"<unused_26>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 31,\n",
      "      \"content\": \"<unused_27>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 32,\n",
      "      \"content\": \"<unused_28>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head -n 300 ./tokenizers/nsmc_bbpe_48000/tokenizer.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with Existing Tokenizer\n",
    "\n",
    "Before you go, you need to see config of tokenizer.\n",
    "\n",
    "You can check the configuration of tokenizer at:\n",
    "https://huggingface.co/klue/bert-base/tree/main\n",
    "\n",
    "or cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 289/289 [00:00<00:00, 31.9kB/s]\n",
      "config.json: 100%|██████████| 425/425 [00:00<00:00, 226kB/s]\n",
      "vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 505kB/s]\n",
      "tokenizer.json: 100%|██████████| 495k/495k [00:00<00:00, 950kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 60.1kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>이것은 테스트 문장입니다. <unused_0>어떻게 보이나요?<unused_1> 고유명사 \"파이썬 파이토치 허깅페이스\"는 어떻게 되나요?</s>\n",
      ">>> [2, 32, 86, 34, 3982, 2073, 7453, 6265, 12190, 18, 32, 15818, 5722, 4948, 66, 20, 34, 3842, 3783, 2075, 2182, 35, 32, 15818, 5722, 4948, 66, 21, 34, 6870, 2211, 2063, 6, 6440, 3495, 6440, 2386, 2225, 1905, 2186, 15092, 6, 793, 3842, 859, 2075, 2182, 35, 32, 19, 86, 34, 3]\n",
      ">>> ['<', 's', '>', '이것', '##은', '테스트', '문장', '##입니다', '.', '<', 'un', '##us', '##ed', '_', '0', '>', '어떻게', '보이', '##나', '##요', '?', '<', 'un', '##us', '##ed', '_', '1', '>', '고유', '##명', '##사', '\"', '파이', '##썬', '파이', '##토', '##치', '허', '##깅', '##페이스', '\"', '는', '어떻게', '되', '##나', '##요', '?', '<', '/', 's', '>']\n",
      ">>> < s > 이것은 테스트 문장입니다. < unused _ 0 > 어떻게 보이나요? < unused _ 1 > 고유명사 \" 파이썬 파이토치 허깅페이스 \" 는 어떻게 되나요? < / s >\n",
      "<s>This is a test sentence. <unused_0>How does it look?<unused_1> Proper nouns \"Python PyTorch HuggingFace\" how does it go?</s>\n",
      ">>> [2, 32, 86, 34, 9796, 4641, 11376, 68, 87, 8119, 17219, 30062, 9963, 18, 32, 15818, 5722, 4948, 66, 20, 34, 25567, 21287, 4080, 16050, 79, 10069, 35, 32, 15818, 5722, 4948, 66, 21, 34, 10966, 10676, 19665, 5830, 2041, 6, 52, 2076, 7088, 3827, 52, 2076, 2081, 4020, 4906, 44, 13721, 2064, 4586, 2314, 11711, 6, 75, 5936, 21287, 4080, 16050, 11970, 35, 32, 19, 86, 34, 3]\n",
      ">>> ['<', 's', '>', 'Th', '##is', 'is', 'a', 't', '##est', 'se', '##nt', '##ence', '.', '<', 'un', '##us', '##ed', '_', '0', '>', 'How', 'do', '##es', 'it', 'l', '##ook', '?', '<', 'un', '##us', '##ed', '_', '1', '>', 'Pro', '##per', 'no', '##un', '##s', '\"', 'P', '##y', '##th', '##on', 'P', '##y', '##T', '##or', '##ch', 'H', '##ug', '##g', '##ing', '##F', '##ace', '\"', 'h', '##ow', 'do', '##es', 'it', 'go', '?', '<', '/', 's', '>']\n",
      ">>> < s > This is a test sentence. < unused _ 0 > How does it look? < unused _ 1 > Proper nouns \" Python PyTorch HuggingFace \" how does it go? < / s >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plm_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(plm_name)\n",
    "\n",
    "test_tokenizer(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
